
@software{jonasnilsberg_contribute_2018,
	title = {Contribute to matistikk development by creating an account on {GitHub}},
	url = {https://github.com/jonasnilsberg/matistikk},
	author = {jonasnilsberg},
	urldate = {2018-03-19},
	date = {2018-03-16},
	note = {original-date: 2017-06-06T13:23:39Z},
	file = {Snapshot:/home/havard/Zotero/storage/H6A2N27W/matistikk.html:text/html}
}

@software{hvass-labs_tensorflow-tutorials:_2018,
	title = {{TensorFlow}-Tutorials: {TensorFlow} Tutorials with {YouTube} Videos},
	rights = {{MIT}},
	url = {https://github.com/Hvass-Labs/TensorFlow-Tutorials},
	shorttitle = {{TensorFlow}-Tutorials},
	author = {Hvass-Labs},
	urldate = {2018-03-06},
	date = {2018-03-06},
	note = {original-date: 2016-06-26T14:43:21Z},
	keywords = {deep-learning, machine-learning, neural-network, python-notebook, reinforcement-learning, tensorflow, tutorial, youtube},
	file = {Snapshot:/home/havard/Zotero/storage/LCUBEU7J/TensorFlow-Tutorials.html:text/html}
}

@article{zhang_drawing_2016,
	title = {Drawing and Recognizing Chinese Characters with Recurrent Neural Network},
	url = {http://arxiv.org/abs/1606.06539},
	abstract = {Recent deep learning based approaches have achieved great success on handwriting recognition. Chinese characters are among the most widely adopted writing systems in the world. Previous research has mainly focused on recognizing handwritten Chinese characters. However, recognition is only one aspect for understanding a language, another challenging and interesting task is to teach a machine to automatically write (pictographic) Chinese characters. In this paper, we propose a framework by using the recurrent neural network ({RNN}) as both a discriminative model for recognizing Chinese characters and a generative model for drawing (generating) Chinese characters. To recognize Chinese characters, previous methods usually adopt the convolutional neural network ({CNN}) models which require transforming the online handwriting trajectory into image-like representations. Instead, our {RNN} based approach is an end-to-end system which directly deals with the sequential structure and does not require any domain-specific knowledge. With the {RNN} system (combining an {LSTM} and {GRU}), state-of-the-art performance can be achieved on the {ICDAR}-2013 competition database. Furthermore, under the {RNN} framework, a conditional generative model with character embedding is proposed for automatically drawing recognizable Chinese characters. The generated characters (in vector format) are human-readable and also can be recognized by the discriminative {RNN} model with high accuracy. Experimental results verify the effectiveness of using {RNNs} as both generative and discriminative models for the tasks of drawing and recognizing Chinese characters.},
	journaltitle = {{arXiv}:1606.06539 [cs]},
	author = {Zhang, Xu-Yao and Yin, Fei and Zhang, Yan-Ming and Liu, Cheng-Lin and Bengio, Yoshua},
	urldate = {2018-03-06},
	date = {2016-06-21},
	eprinttype = {arxiv},
	eprint = {1606.06539},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1606.06539 PDF:/home/havard/Zotero/storage/WTS4BK54/Zhang et al. - 2016 - Drawing and Recognizing Chinese Characters with Re.pdf:application/pdf;arXiv.org Snapshot:/home/havard/Zotero/storage/FV7N49WU/1606.html:text/html}
}

@inproceedings{mouchere_icfhr2016_2016,
	title = {{ICFHR}2016 {CROHME}: Competition on Recognition of Online Handwritten Mathematical Expressions},
	doi = {10.1109/ICFHR.2016.0116},
	abstract = {This paper presents an overview of the 5th Competition on Recognition of Online Handwritten Mathematical Expressions ({CROHME}). As in previous years, the main task is formula recognition from handwritten strokes (Task 1). Additional tasks include classification of isolated symbols (Task 2a), classification of isolated valid and invalid symbols (Task 2b), a new task on parsing formula structure from valid handwritten symbols (Task 3), and parsing expressions with matrices (Task 4, experimental). In total, eleven (11) research labs registered for the competition, with six (6) teams submitting results. Innovations for this {CROHME} included providing a corpus of formulae from Wikipedia to train language models, and an online system for result submission. The highest recognition rates were obtained by {MyScript} corporation (Task 1. 67.65\%, 2a. 92.81\%, 2b. 86.77\%, 3. 84.38\%, and 4. 68.40\%). Using only provided training data, the highest recognition rates were obtained by {WIRIS} corporation (Task 1. 49.61\%, Task 3. 78.80\%, Task 4. 56.40\%), the Tokyo University of Agriculture and Technology (Task 2a. 92.28\%), and {RIT} (Task 2b. 83.34\%). The competition results suggest that recognition of handwritten formulae remains a difficult structural pattern recognition task.},
	pages = {607--612},
	booktitle = {2016 15th International Conference on Frontiers in Handwriting Recognition ({ICFHR})},
	author = {Mouchère, H. and Viard-Gaudin, C. and Zanibbi, R. and Garain, U.},
	date = {2016-10},
	keywords = {Electronic publishing, Encyclopedias, formula recognition, formula structure parsing, handwriting recognition, Handwriting recognition, handwritten character recognition, {ICFHR}2016 {CROHME}, image classification, Internet, isolated symbol classification, mathematical expression recognition, mathematics computing, Measurement, {MyScript} corporation, online handwritten mathematical expression recognition, performance evaluation, {RIT}, spatial relations, structural pattern recognition, Tokyo University of Agriculture and Technology, Training, {WIRIS} corporation}
}

@inproceedings{dai_nguyen_deep_2015,
	title = {Deep neural networks for recognizing online handwritten mathematical symbols},
	pages = {121--125},
	booktitle = {Pattern Recognition ({ACPR}), 2015 3rd {IAPR} Asian Conference on},
	publisher = {{IEEE}},
	author = {Dai Nguyen, Hai and Le, Anh Duc and Nakagawa, Masaki},
	date = {2015},
	file = {HaiDNguyenACPR2015.pdf:/home/havard/Zotero/storage/5CDMTQA8/HaiDNguyenACPR2015.pdf:application/pdf}
}

@article{peddada_eqnmaster_????,
	title = {{EqnMaster}},
	url = {https://cs224d.stanford.edu/reports/PeddadaAmani.pdf},
	abstract = {Evaluating Mathematical Expressions with Generative Recurrent Networks},
	author = {Peddada, Amani V. and Tsang, Arthur L.},
	file = {PeddadaAmani.pdf:/home/havard/Zotero/storage/SGMR7E7C/PeddadaAmani.pdf:application/pdf}
}

@online{chee_ink_2011,
	title = {Ink Markup Language ({InkML})},
	url = {https://www.w3.org/TR/InkML/},
	shorttitle = {Ink Markup Language},
	titleaddon = {Ink Markup Language ({InkML})},
	type = {W3C Recommendation 20 September 2011},
	author = {Chee, Yi-Min and Franke, Katrin and Froumentin, Max and Madhvanath, Sriganesh and Magaña, Jose-Antonio and Pakosz, Grégory and Russell, Gregory and Muthuselvam, Selvaraj and Seni, Giovanni and Tremblay, Christopher and Yaeger, Larry},
	urldate = {2018-03-06},
	date = {2011-09-20},
	file = {Ink Markup Language (InkML):/home/havard/Zotero/storage/33KJX96F/InkML.html:text/html}
}

@book{nielsen_neural_2015,
	title = {Neural Networks and Deep Learning},
	url = {http://neuralnetworksanddeeplearning.com},
	publisher = {Determination Press},
	author = {Nielsen, Michael A.},
	urldate = {2018-03-06},
	date = {2015},
	langid = {english},
	file = {Snapshot:/home/havard/Zotero/storage/GXY886NM/neuralnetworksanddeeplearning.com.html:text/html}
}

@article{abadi_tensorflow:_2016,
	title = {Tensorflow: Large-scale machine learning on heterogeneous distributed systems},
	shorttitle = {Tensorflow},
	journaltitle = {{arXiv} preprint {arXiv}:1603.04467},
	author = {Abadi, Martín and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu},
	date = {2016},
	file = {45166.pdf:/home/havard/Zotero/storage/ZG9L9Y8N/45166.pdf:application/pdf}
}

@online{_tensorflow_????,
	title = {{TensorFlow}},
	url = {https://www.tensorflow.org/},
	abstract = {An open-source machine learning framework for everyone},
	titleaddon = {{TensorFlow}},
	urldate = {2018-03-06},
	langid = {english},
	file = {Snapshot:/home/havard/Zotero/storage/2AJV646J/www.tensorflow.org.html:text/html}
}

@online{_keras_????,
	title = {Keras Documentation},
	url = {https://keras.io/},
	urldate = {2018-03-06},
	file = {Keras Documentation:/home/havard/Zotero/storage/BY4YGTT2/keras.io.html:text/html}
}

@online{_all_????,
	title = {All symbols in {TensorFlow}},
	url = {https://www.tensorflow.org/api_docs/python/},
	titleaddon = {{TensorFlow}},
	urldate = {2018-03-06},
	langid = {english},
	file = {Snapshot:/home/havard/Zotero/storage/9Z68PXEN/python.html:text/html}
}

@article{thoma_-line_2015,
	title = {On-line Recognition of Handwritten Mathematical Symbols},
	url = {http://arxiv.org/abs/1511.09030},
	doi = {10.5445/IR/1000048047},
	abstract = {Finding the name of an unknown symbol is often hard, but writing the symbol is easy. This bachelor's thesis presents multiple systems that use the pen trajectory to classify handwritten symbols. Five preprocessing steps, one data augmentation algorithm, five features and five variants for multilayer Perceptron training were evaluated using 166898 recordings which were collected with two crowdsourcing projects. The evaluation results of these 21 experiments were used to create an optimized recognizer which has a {TOP}1 error of less than 17.5\% and a {TOP}3 error of 4.0\%. This is an improvement of 18.5\% for the {TOP}1 error and 29.7\% for the {TOP}3 error.},
	journaltitle = {{arXiv}:1511.09030 [cs]},
	author = {Thoma, Martin},
	urldate = {2018-03-06},
	date = {2015-11-29},
	eprinttype = {arxiv},
	eprint = {1511.09030},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1511.09030 PDF:/home/havard/Zotero/storage/64WZQMVT/Thoma - 2015 - On-line Recognition of Handwritten Mathematical Sy.pdf:application/pdf;arXiv.org Snapshot:/home/havard/Zotero/storage/4ULI9GCT/1511.html:text/html}
}

@online{_tornado.testing_????,
	title = {tornado.testing — Unit testing support for asynchronous code — Tornado 5.0.1 documentation},
	url = {http://www.tornadoweb.org/en/stable/testing.html},
	urldate = {2018-04-03},
	file = {tornado.testing — Unit testing support for asynchronous code — Tornado 5.0.1 documentation:/home/havard/Zotero/storage/36IQ7R6E/testing.html:text/html}
}

@online{_what_????,
	title = {What is Proof Of Concept? definition and meaning},
	url = {http://www.investorwords.com/3899/proof_of_concept.html},
	shorttitle = {What is Proof Of Concept?},
	abstract = {Definition of proof of concept: Evidence that demonstrates that a business model or idea is feasible.},
	titleaddon = {{InvestorWords}.com},
	urldate = {2018-04-08},
	file = {Snapshot:/home/havard/Zotero/storage/3A76RXPL/proof_of_concept.html:text/html}
}

@book{murphy_machine_2012,
	location = {Cambridge, {MA}},
	title = {Machine learning: a probabilistic perspective},
	isbn = {978-0-262-01802-9},
	series = {Adaptive computation and machine learning series},
	shorttitle = {Machine learning},
	pagetotal = {1067},
	publisher = {{MIT} Press},
	author = {Murphy, Kevin P.},
	date = {2012},
	langid = {english},
	keywords = {Machine learning, Probabilities},
	file = {Murphy - 2012 - Machine learning a probabilistic perspective.pdf:/home/havard/Zotero/storage/SE6D34KD/Murphy - 2012 - Machine learning a probabilistic perspective.pdf:application/pdf}
}

@online{_presse_????,
	title = {Presse - {YouTube}},
	url = {https://www.youtube.com/intl/no/yt/about/press/},
	urldate = {2018-04-08},
	file = {Presse - YouTube:/home/havard/Zotero/storage/N7665RFH/press.html:text/html;Presse - YouTube.pdf:/home/havard/Zotero/storage/ICAAQBAH/Presse - YouTube.pdf:application/pdf}
}

@book{murphy_machine_2012-1,
	location = {Cambridge, {MA}},
	title = {Machine learning: a probabilistic perspective},
	isbn = {978-0-262-01802-9},
	series = {Adaptive computation and machine learning series},
	shorttitle = {Machine learning},
	pagetotal = {1067},
	publisher = {{MIT} Press},
	author = {Murphy, Kevin P.},
	date = {2012},
	langid = {english},
	keywords = {Machine learning, Probabilities},
	file = {Murphy - 2012 - Machine learning a probabilistic perspective.pdf:/home/havard/Zotero/storage/DTB7T7GE/Murphy - 2012 - Machine learning a probabilistic perspective.pdf:application/pdf}
}

@online{_youtube_2016,
	title = {Youtube Statistics - 2018},
	url = {https://fortunelords.com/youtube-statistics/},
	abstract = {The latest Youtube statistics for 2018 which are going to blow your mind.},
	titleaddon = {Digital Marketing Education},
	urldate = {2018-04-08},
	date = {2016-03-19},
	langid = {american},
	file = {Snapshot:/home/havard/Zotero/storage/ZR9UFBY3/youtube-statistics.html:text/html}
}

@inproceedings{cline_predictive_2017,
	title = {Predictive maintenance applications for machine learning},
	doi = {10.1109/RAM.2017.7889679},
	pages = {1--7},
	booktitle = {2017 Annual Reliability and Maintainability Symposium ({RAMS})},
	author = {Cline, B. and Niculescu, R. S. and Huffman, D. and Deckel, B.},
	date = {2017-01},
	keywords = {Analytical models, Connectors, Data models, environmental variable, failure analysis, failure signature, Inspection, learning (artificial intelligence), machine learning, Machine Learning, machine performance, maintenance engineering, maintenance planning, planning, Predicted Failure Analysis, predictive maintenance, Predictive maintenance, Predictive Maintenance, Predictive models, production engineering computing, reliability}
}

@article{srivastava_dropout:_2014,
	title = {Dropout: A Simple Way to Prevent Neural Networks from Overﬁtting},
	abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overﬁtting is a serious problem in such networks. Large networks are also slow to use, making it diﬃcult to deal with overﬁtting by combining the predictions of many diﬀerent large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of diﬀerent “thinned” networks. At test time, it is easy to approximate the eﬀect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This signiﬁcantly reduces overﬁtting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classiﬁcation and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
	pages = {30},
	author = {Srivastava, Nitish and Hinton, Geoﬀrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	date = {2014-06},
	langid = {english},
	file = {Srivastava et al. - Dropout A Simple Way to Prevent Neural Networks f.pdf:/home/havard/Zotero/storage/V5L9T4N9/Srivastava et al. - Dropout A Simple Way to Prevent Neural Networks f.pdf:application/pdf}
}

@article{xie_recognition_????,
	title = {On the Recognition of Handwritten Mathematical Symbols},
	abstract = {We have designed and implemented a handwritten mathematical symbol recognition system that includes two recognizers: an elastic matching-based recognizer and a hidden Markov model-based recognizer. The architecture includes data preprocessing, data analysis, symbol representation and recognition.},
	pages = {152},
	author = {Xie, Xiaofang},
	langid = {english},
	file = {Xie - On the Recognition of Handwritten Mathematical Sym.pdf:/home/havard/Zotero/storage/Q6FSV8KD/Xie - On the Recognition of Handwritten Mathematical Sym.pdf:application/pdf}
}

@book{martin_abadi_tensorflow:_2015,
	title = {{TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
	url = {https://www.tensorflow.org/},
	author = {{Martín Abadi} and {Ashish Agarwal} and {Paul Barham} and {Eugene Brevdo} and {Zhifeng Chen} and {Craig Citro} and {Greg S. Corrado} and {Andy Davis} and {Jeffrey Dean} and {Matthieu Devin} and {Sanjay Ghemawat} and {Ian Goodfellow} and {Andrew Harp} and {Geoffrey Irving} and {Michael Isard} and Jia, Yangqing and {Rafal Jozefowicz} and {Lukasz Kaiser} and {Manjunath Kudlur} and {Josh Levenberg} and {Dandelion Mané} and {Rajat Monga} and {Sherry Moore} and {Derek Murray} and {Chris Olah} and {Mike Schuster} and {Jonathon Shlens} and {Benoit Steiner} and {Ilya Sutskever} and {Kunal Talwar} and {Paul Tucker} and {Vincent Vanhoucke} and {Vijay Vasudevan} and {Fernanda Viégas} and {Oriol Vinyals} and {Pete Warden} and {Martin Wattenberg} and {Martin Wicke} and {Yuan Yu} and {Xiaoqiang Zheng}},
	date = {2015}
}

@article{simon_off-line_1992,
	title = {Off-line cursive word recognition},
	volume = {80},
	issn = {0018-9219},
	doi = {10.1109/5.156476},
	abstract = {The state of the art in handwriting recognition, especially in cursive word recognition, is surveyed, and some basic notions are reviewed in the field of picture recognition, particularly, line image recognition. The usefulness of `regular' versus `singular' classes of features is stressed. These notions are applied to obtain a graph, G , representing a line image, and also to find an `axis' as the regular part of G. The complements to G of the axis are the `tarsi', singular parts of G, which correspond to informative features of a cursive word. A segmentation of the graph is obtained, giving a symbolic description chain ({SDC}). Using one or more as robust anchors, possible words in a list of words are selected. Candidate words are examined to see if the other letters fit the rest of the {SDC}. Good results are obtained for clean images of words written by several persons},
	pages = {1150--1161},
	number = {7},
	journaltitle = {Proceedings of the {IEEE}},
	author = {Simon, J. C.},
	date = {1992-07},
	keywords = {handwriting recognition, Handwriting recognition, Background noise, Character recognition, cursive word recognition, graph, graph theory, Graphics, Image recognition, Image segmentation, line image recognition, optical character recognition, Pattern recognition, picture recognition, Pixel, Robustness, segmentation, Speech recognition, symbolic description chain},
	file = {IEEE Xplore Abstract Record:/home/havard/Zotero/storage/TSYLQZUX/156476.html:text/html}
}

@software{_activation_????,
	title = {Activation functions},
	url = {https://gist.github.com/yusugomori/cf7bce19b8e16d57488a}
}

@software{_python_????,
	title = {Python implementation of the Ramer-Douglas-Peucker algorithm},
	url = {https://github.com/fhirschmann/rdp/blob/master/rdp/__init__.py}
}

@online{_neural_2018,
	title = {Neural Networks Part 1: Setting up the Architecture},
	url = {http://cs231n.github.io/neural-networks-1/},
	titleaddon = {cs231n.github.io},
	date = {2018-09-04}
}

@online{_multi-layer_????,
	title = {Multi-Layer Neural Network},
	url = {http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/},
	titleaddon = {stanfort.edu}
}

@article{lu_recognition_2015,
	title = {Recognition of Online Handwritten Mathematical Expressions Using Convolutional Neural Networks},
	abstract = {We further investigate the problem of recognizing handwritten mathematical expressions, which we also chose for our {CS}221 ﬁnal project [3]. Being able to change handwritten expressions into {LATEX} has applications for consumers and academics. While large amounts of work have been done for digit and character recognition [2] [10], much less progress has been made surrounding handwritten expression recognition. To the best of our knowledge, no papers have been published applying Convolutional Neural Networks ({CNNs}) to the task of handwritten expression recognition. We build an end-to-end system using our best {CNN} model to go from strokes to symbols to a {LATEX} expression. We also compare our results to other systems; experimental evaluation suggests that {CNNs} are a powerful tool for handwritten mathematical expression recognition.},
	pages = {7},
	author = {Lu, Catherine and Mohan, Karanveer},
	date = {2015},
	langid = {english},
	file = {Lu and Mohan - Recognition of Online Handwritten Mathematical Exp.pdf:/home/havard/Zotero/storage/2W4QHNNT/Lu and Mohan - Recognition of Online Handwritten Mathematical Exp.pdf:application/pdf}
}

@inproceedings{priya_online_2016,
	title = {Online and offline character recognition: A survey},
	isbn = {978-1-5090-0396-9},
	url = {http://ieeexplore.ieee.org/document/7754291/},
	doi = {10.1109/ICCSP.2016.7754291},
	shorttitle = {Online and offline character recognition},
	abstract = {Handwritten character recognition has been one of the most fascinating research among the various researches in field of image processing. In Handwritten character recognition method the input is scanned from images, documents and real time devices like tablets, tabloids, digitizers etc. which are then interpreted into digital text. There are basically two approaches - Online Handwritten recognition which takes the input at run time and Offline Handwritten Recognition which works on scanned images. In this paper we have discussed the architecture, the steps involved, and the various proposed methodologies of offline and online character recognition along with their comparison and few applications.},
	pages = {0967--0970},
	publisher = {{IEEE}},
	author = {Priya, Anisha and Mishra, Surbhi and Raj, Saloni and Mandal, Sudarshan and Datta, Sujoy},
	urldate = {2018-04-10},
	date = {2016-04},
	langid = {english},
	file = {Priya et al. - 2016 - Online and offline character recognition A survey.pdf:/home/havard/Zotero/storage/XTPS88BI/Priya et al. - 2016 - Online and offline character recognition A survey.pdf:application/pdf}
}

@online{gibbs_googles_2018,
	title = {Google’s {AI} is being used by {US} military drone programme},
	url = {http://www.theguardian.com/technology/2018/mar/07/google-ai-us-department-of-defense-military-drone-project-maven-tensorflow},
	abstract = {{DoD}’s Project Maven uses tech firm’s {TensorFlow} artificial intelligence systems, prompting debate both inside and outside company},
	titleaddon = {the Guardian},
	author = {Gibbs, Samuel},
	urldate = {2018-04-10},
	date = {2018-03-07},
	langid = {english},
	file = {Snapshot:/home/havard/Zotero/storage/G67A5HC7/google-ai-us-department-of-defense-military-drone-project-maven-tensorflow.html:text/html}
}

@article{mori_historical_1992,
	title = {Historical review of {OCR} research and development},
	volume = {80},
	issn = {00189219},
	url = {http://ieeexplore.ieee.org/document/156468/},
	doi = {10.1109/5.156468},
	pages = {1029--1058},
	number = {7},
	journaltitle = {Proceedings of the {IEEE}},
	author = {Mori, S. and Suen, C.Y. and Yamamoto, K.},
	urldate = {2018-04-10},
	date = {1992-07},
	langid = {english},
	file = {Mori et al. - 1992 - Historical review of OCR research and development.pdf:/home/havard/Zotero/storage/2VY2MLX7/Mori et al. - 1992 - Historical review of OCR research and development.pdf:application/pdf}
}

@incollection{smith_scientist_1997,
	title = {The Scientist and Engineer's Guide to Digital Signal Processing},
	isbn = {{ISBN} 0-9660176-3-3},
	booktitle = {The Scientist and Engineer's Guide to Digital Signal Processing},
	publisher = {California Technical Publishing},
	author = {Smith, Steven. W},
	date = {1997}
}

@online{_icfhr_????,
	title = {{ICFHR} 2018},
	url = {http://icfhr2018.org/index.html},
	urldate = {2018-04-10},
	file = {ICFHR 2018:/home/havard/Zotero/storage/JFEZMNVM/index.html:text/html}
}

@article{mouchere_advancing_2016,
	title = {Advancing the state of the art for handwritten math recognition: the {CROHME} competitions, 2011–2014},
	volume = {19},
	issn = {1433-2833, 1433-2825},
	url = {http://link.springer.com/10.1007/s10032-016-0263-5},
	doi = {10.1007/s10032-016-0263-5},
	shorttitle = {Advancing the state of the art for handwritten math recognition},
	abstract = {The {CROHME} competitions have helped organize the ﬁeld of handwritten mathematical expression recognition. This paper presents the evolution of the competition over its ﬁrst 4 years, and its contributions to handwritten math recognition, and more generally structural pattern recognition research. The competition protocol, evaluation metrics and datasets are presented in detail. Participating systems are analyzed and compared in terms of the central mathematical expression recognition tasks: (1) symbol segmentation, (2) classiﬁcation of individual symbols, (3) symbol relationships and (4) structural analysis (parsing). The competition led to the development of label graphs, which allow recognition results with conﬂicting segmentations to be directly compared and quantiﬁed using Hamming distances. We introduce structure confusion histograms that provide frequencies for incorrect subgraphs corresponding to ground-truth label subgraphs of a given size and present structure confusion histograms for symbol bigrams (two symbols with a relationship) for {CROHME} 2014 systems.},
	pages = {173--189},
	number = {2},
	journaltitle = {International Journal on Document Analysis and Recognition ({IJDAR})},
	author = {Mouchère, Harold and Zanibbi, Richard and Garain, Utpal and Viard-Gaudin, Christian},
	urldate = {2018-04-10},
	date = {2016-06},
	langid = {english},
	file = {Mouchère et al. - 2016 - Advancing the state of the art for handwritten mat.pdf:/home/havard/Zotero/storage/VXCX6CIJ/Mouchère et al. - 2016 - Advancing the state of the art for handwritten mat.pdf:application/pdf}
}

@online{_introduction_2018,
	title = {Introduction to {LaTeX}},
	url = {https://www.latex-project.org/about/},
	urldate = {2018-04-11},
	date = {2018},
	file = {Introduction to LaTeX:/home/havard/Zotero/storage/5T8QWR9K/about.html:text/html}
}

@online{_mit_????,
	title = {The {MIT} License {\textbar} Open Source Initiative},
	url = {https://opensource.org/licenses/MIT},
	urldate = {2018-04-12},
	file = {The MIT License | Open Source Initiative:/home/havard/Zotero/storage/PJ5MMQ98/MIT.html:text/html}
}

@inproceedings{priya_online_2016-1,
	title = {Online and offline character recognition: A survey},
	doi = {10.1109/ICCSP.2016.7754291},
	shorttitle = {Online and offline character recognition},
	abstract = {Handwritten character recognition has been one of the most fascinating research among the various researches in field of image processing. In Handwritten character recognition method the input is scanned from images, documents and real time devices like tablets, tabloids, digitizers etc. which are then interpreted into digital text. There are basically two approaches - Online Handwritten recognition which takes the input at run time and Offline Handwritten Recognition which works on scanned images. In this paper we have discussed the architecture, the steps involved, and the various proposed methodologies of offline and online character recognition along with their comparison and few applications.},
	eventtitle = {2016 International Conference on Communication and Signal Processing ({ICCSP})},
	pages = {0967--0970},
	booktitle = {2016 International Conference on Communication and Signal Processing ({ICCSP})},
	author = {Priya, A. and Mishra, S. and Raj, S. and Mandal, S. and Datta, S.},
	date = {2016-04},
	keywords = {Handwriting recognition, handwritten character recognition, Training, Character recognition, Computers, digital text, Feature extraction, Handwritten recognition, image processing, image recognition, Neural networks, offline character recognition, online character recognition, scanned images, text detection, Writing},
	file = {IEEE Xplore Abstract Record:/home/havard/Zotero/storage/GZEAKUQT/7754291.html:text/html}
}

@online{_bachelor_????,
	title = {Bachelor 10 - Online {LaTeX} Editor {ShareLaTeX}},
	url = {https://www.sharelatex.com/project/5a7ad845533c9257327735b1},
	urldate = {2018-04-12},
	file = {Bachelor 10 - Online LaTeX Editor ShareLaTeX:/home/havard/Zotero/storage/5BKWPYB9/5a7ad845533c9257327735b1.pdf:application/pdf}
}

@inproceedings{steinkraus_using_2005,
	title = {Using {GPUs} for machine learning algorithms},
	isbn = {978-0-7695-2420-7},
	url = {http://ieeexplore.ieee.org/document/1575717/},
	doi = {10.1109/ICDAR.2005.251},
	abstract = {Using dedicated hardware to do machine learning typically ends up in disaster because of cost, obsolescence, and poor software. The popularization of Graphic Processing Units ({GPUs}), which are now available on every {PC}, provides an attractive alternative. We propose a generic 2-layer fully connected neural network {GPU} implementation which yields over 3X speedup for both training and testing with respect to a 3GHz P4 {CPU}.},
	pages = {1115--1120 Vol. 2},
	publisher = {{IEEE}},
	author = {Steinkraus, D. and Buck, I. and Simard, P.Y.},
	urldate = {2018-04-16},
	date = {2005},
	langid = {english},
	file = {Steinkraus et al. - 2005 - Using GPUs for machine learning algorithms.pdf:/home/havard/Zotero/storage/AWFAE4BQ/Steinkraus et al. - 2005 - Using GPUs for machine learning algorithms.pdf:application/pdf}
}

@article{werbos_beyond_1974,
	title = {Beyond regression : new tools for prediction and analysis in the behavioral sciences /},
	author = {Werbos, Paul and J. (Paul John, Paul},
	date = {1974}
}

@incollection{drenick_applications_1982,
	location = {Berlin/Heidelberg},
	title = {Applications of advances in nonlinear sensitivity analysis},
	volume = {38},
	isbn = {978-3-540-11691-2},
	url = {http://www.springerlink.com/index/10.1007/BFb0006203},
	pages = {762--770},
	booktitle = {System Modeling and Optimization},
	publisher = {Springer-Verlag},
	author = {Werbos, Paul J.},
	editor = {Drenick, R. F. and Kozin, F.},
	urldate = {2018-04-16},
	date = {1982},
	langid = {english},
	doi = {10.1007/BFb0006203},
	file = {Werbos - 1982 - Applications of advances in nonlinear sensitivity .pdf:/home/havard/Zotero/storage/L5DAEGFU/Werbos - 1982 - Applications of advances in nonlinear sensitivity .pdf:application/pdf}
}

@online{_datasets_????,
	title = {Datasets - Keras Documentation},
	url = {https://keras.io/datasets/},
	urldate = {2018-04-16},
	file = {Datasets - Keras Documentation:/home/havard/Zotero/storage/DG5E2BPR/datasets.html:text/html}
}

@article{rumelhart_learning_1986,
	title = {Learning representations by back-propagating errors},
	volume = {323},
	url = {http://dx.doi.org/10.1038/323533a0},
	pages = {533},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	date = {1986-10-09},
	file = {backprop_old.pdf:/home/havard/Zotero/storage/GSDZQGLZ/backprop_old.pdf:application/pdf}
}

@article{nielsen_neural_2015-1,
	title = {Neural Networks and Deep Learning},
	url = {http://neuralnetworksanddeeplearning.com},
	author = {Nielsen, Michael A.},
	urldate = {2018-04-17},
	date = {2015},
	langid = {english},
	file = {Snapshot:/home/havard/Zotero/storage/9I7JY7CE/chap6.html:text/html}
}

@online{brownlee_what_2017,
	title = {What is the Difference Between a Parameter and a Hyperparameter?},
	url = {https://machinelearningmastery.com/difference-between-a-parameter-and-a-hyperparameter/},
	abstract = {It can be confusing when you get started in applied machine learning. There are so many terms to use and many of the terms may not be used consistently. This is especially true if you have come from another field of study that may use some of the same terms as machine learning, but they …},
	titleaddon = {Machine Learning Mastery},
	author = {Brownlee, Jason},
	urldate = {2018-04-17},
	date = {2017-07-26},
	langid = {american},
	file = {Snapshot:/home/havard/Zotero/storage/E3PN25VX/difference-between-a-parameter-and-a-hyperparameter.html:text/html}
}

@book{goodfellow_deep_2016,
	title = {Deep Learning},
	publisher = {{MIT} Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	date = {2016}
}

@inproceedings{zhou_computation_1988,
	title = {Computation of optical flow using a neural network},
	doi = {10.1109/ICNN.1988.23914},
	abstract = {A method for computing optical flow using a neural network is presented. Usually, the measurement primitives used for computing optical flow from successive image frames are the image-intensity values and their spatial and temporal derivatives, and tokens such as edges, corners, and linear features. Conventional methods based on such primitives suffer from edge sparsity, noise distortion, or sensitivity to rotation. The authors first fit a 2-D polynomial to find a smooth continuous image-intensity function in a window and estimate the subpixel intensity values and their principal curvatures. Under the local rigidity assumption and smoothness constraints, a neural network is then used to implement the computing procedure based on the estimated intensity values and their principal curvatures. Owing to the dense measured primitives, a dense optical flow with subpixel accuracy is obtained with only a few iterations. Since intensity values and their principle curvatures are rotation-invariant, this method can detect both rotating and translating objects in the scene. Experimental results using synthetic image sequences demonstrate the efficacy of the method.{\textless}{\textgreater}},
	eventtitle = {{IEEE} 1988 International Conference on Neural Networks},
	pages = {71--78 vol.2},
	booktitle = {{IEEE} 1988 International Conference on Neural Networks},
	author = {Zhou, Y. T. and Chellappa, R.},
	date = {1988-07},
	keywords = {Neural networks, 2-D polynomial, computerised picture processing, Image processing, neural nets, neural network, optical flow, smooth continuous image-intensity function, synthetic image sequences},
	file = {IEEE Xplore Abstract Record:/home/havard/Zotero/storage/L92LWBM2/23914.html:text/html}
}

@article{fukushima_handwritten_????,
	title = {Handwritten Alphanumeric Character Recognition by the Neocognitron},
	abstract = {A neural network model of visual pattern recognition, called the neocognitron, was previously proposed by one of the present authors. It is capable of deformation-invariant visual pattern recognition. In this paper, we discuss a patternrecognition system which works with the mechanism of the neocognitron. The system has been trained to recognize 35 handwritten alphanumeric characters. The ability to correctly recognize deformed characters depends highly on the choice of the training pattern set. This paper offers some techniques for selecting training patterns useful for deformation-invariant recognition of a large number of characters.},
	pages = {11},
	author = {Fukushima, Kunihiko and Wake, Nobuaki},
	langid = {english},
	file = {Fukushima and Wake - Handwritten Alphanumeric Character Recognition by .pdf:/home/havard/Zotero/storage/H6JCV4FF/Fukushima and Wake - Handwritten Alphanumeric Character Recognition by .pdf:application/pdf}
}

@article{w._smith_neural_????,
	title = {Neural Network Architecture},
	url = {http://www.dspguide.com/ch26/2.htm},
	author = {W. Smith, Steven},
	urldate = {2018-04-20},
	file = {Neural Network Architecture:/home/havard/Zotero/storage/LDYXK3QL/2.html:text/html}
}

@online{_cs231n_????,
	title = {{CS}231n Convolutional Neural Networks for Visual Recognition - Neural Networks 1},
	url = {http://cs231n.github.io/neural-networks-1/},
	urldate = {2018-04-20},
	file = {CS231n Convolutional Neural Networks for Visual Recognition:/home/havard/Zotero/storage/BS2YSCSD/neural-networks-1.html:text/html}
}

@article{jain_artificial_1996,
	title = {Artificial neural networks: a tutorial},
	volume = {29},
	issn = {0018-9162},
	doi = {10.1109/2.485891},
	pages = {31--44},
	number = {3},
	journaltitle = {Computer},
	author = {Jain, A. K. and Mao, Jianchang and Mohiuddin, K. M.},
	date = {1996-03},
	keywords = {neural nets, {ANN} application, artificial computational model, artificial neural networks, Artificial neural networks, basic biological neuron, Biological neural networks, Biological system modeling, Biology computing, character recognition, Computer architecture, Concurrent computing, Humans, Integrated circuit interconnections, interconnected simple processors, learning processes, massively parallel systems, network architectures, neurophysiology, Parallel processing, Tutorial}
}

@article{mcculloch_logical_1943,
	title = {A logical calculus of the ideas immanent in nervous activity},
	volume = {5},
	pages = {115--133},
	number = {4},
	journaltitle = {The bulletin of mathematical biophysics},
	author = {{McCulloch}, Warren S and Pitts, Walter},
	date = {1943}
}

@online{_neural_????,
	title = {Neural Network Architecture},
	url = {http://www.dspguide.com/ch26/2.htm},
	urldate = {2018-04-20},
	file = {Neural Network Architecture:/home/havard/Zotero/storage/ERN2BCLC/2.html:text/html}
}

@book{smith_scientist_1997-1,
	title = {The Scientist and Engineer's Guide to Digital Signal Processing},
	url = {http://www.dspguide.com},
	publisher = {California Technical Publishing},
	author = {Smith, Steven W.},
	date = {1997},
	keywords = {book dsp imported}
}

@book{sharma_understanding_2018,
	title = {Understanding Activation Functions in Deep Learning {\textbar} Learn {OpenCV}},
	url = {https://www.learnopencv.com/understanding-activation-functions-in-deep-learning/},
	author = {Sharma, Aditya},
	date = {2018}
}

@book{chollet_keras_2015,
	title = {Keras},
	url = {https://keras.io},
	author = {Chollet, François and {others}},
	date = {2015}
}

@article{hahnloser_digital_2000,
	title = {Digital selection and analogue amplification coexist in a cortex-inspired silicon circuit},
	volume = {405},
	url = {http://dx.doi.org/10.1038/35016072},
	pages = {947},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {Hahnloser, Richard H. R. and Sarpeshkar, Rahul and Mahowald, Misha A. and Douglas, Rodney J. and Seung, H. Sebastian},
	date = {2000-06-22}
}

@inproceedings{zeiler_rectified_2013,
	title = {On rectified linear units for speech processing},
	doi = {10.1109/ICASSP.2013.6638312},
	pages = {3517--3521},
	booktitle = {2013 {IEEE} International Conference on Acoustics, Speech and Signal Processing},
	author = {Zeiler, M. D. and Ranzato, M. and Monga, R. and Mao, M. and Yang, K. and Le, Q. V. and Nguyen, P. and Senior, A. and Vanhoucke, V. and Dean, J. and Hinton, G. E.},
	date = {2013-05},
	keywords = {Training, Neural networks, neural nets, Accuracy, acoustic modeling, Deep Learning, deep neural networks, discriminative tasks, distributed environment, Encoding, Error analysis, gold standard, Hybrid System, key computational unit, large vocabulary speech recognition task, linear projection, logistic function, logistic units, Logistics, Neural Networks, point-wise nonlinearity, rectified linear units, Rectified Linear units, sparse features, speech processing, speech recognition, supervised setting, Unsupervised learning, Unsupervised Learning, vocabulary, word error rates}
}

@incollection{krizhevsky_imagenet_2012,
	title = {{ImageNet} Classification with Deep Convolutional Neural Networks},
	url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
	pages = {1097--1105},
	booktitle = {Advances in Neural Information Processing Systems 25},
	publisher = {Curran Associates, Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	editor = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
	date = {2012}
}

@online{_cs231n_????-1,
	title = {{CS}231n Convolutional Neural Networks for Visual Recognition - Linear classify},
	url = {http://cs231n.github.io/linear-classify/},
	urldate = {2018-04-23}
}

@article{leshno_multilayer_1993,
	title = {Multilayer feedforward networks with a nonpolynomial activation function can approximate any function},
	volume = {6},
	pages = {861--867},
	number = {6},
	journaltitle = {Neural networks},
	author = {Leshno, Moshe and Lin, Vladimir Ya and Pinkus, Allan and Schocken, Shimon},
	date = {1993}
}

@inproceedings{huang_preprocessing_2007,
	title = {Preprocessing Techniques for Online Handwriting Recognition},
	isbn = {978-0-7695-2976-9},
	url = {http://ieeexplore.ieee.org/document/4389705/},
	doi = {10.1109/ISDA.2007.31},
	abstract = {This paper proposes a new preprocessing technique for online handwriting. The approach is to ﬁrst remove the hooks of the strokes by using changed-angle threshold with length threshold, then ﬁlter the noise by using a smoothing technique, which is the combination of the Cubic Spline and the equal-interpolation methods. Finally, the handwriting is normalised. Experiments are carried out using the benchmark {UNIPEN} database. The experimental results show that our preprocessing technique can improve the recognition rates by at least 10\%.},
	pages = {793--800},
	publisher = {{IEEE}},
	author = {Huang, B. Q. and Zhang, Y. B. and Kechadi, M. T.},
	urldate = {2018-04-24},
	date = {2007-10},
	langid = {english},
	file = {Huang et al. - 2007 - Preprocessing Techniques for Online Handwriting Re.pdf:/home/havard/Zotero/storage/N9JF46M4/Huang et al. - 2007 - Preprocessing Techniques for Online Handwriting Re.pdf:application/pdf}
}

@article{gers_learning_1999,
	title = {Learning to Forget: Continual Prediction with {LSTM}},
	volume = {12},
	pages = {2451--2471},
	journaltitle = {Neural Computation},
	author = {Gers, Felix A. and Schmidhuber, Jürgen and Cummins, Fred},
	date = {1999}
}

@article{hochreiter_long_1997,
	title = {Long Short-term Memory},
	volume = {9},
	pages = {1735--80},
	journaltitle = {Neural computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	date = {1997}
}

@online{_understanding_????,
	title = {Understanding {LSTM} Networks -- colah's blog},
	url = {http://colah.github.io/posts/2015-08-Understanding-LSTMs/},
	urldate = {2018-04-24}
}

@online{_handwritten_????,
	title = {Handwritten math symbols dataset},
	url = {https://www.kaggle.com/xainano/handwrittenmathsymbols},
	abstract = {Over 100 000 image samples.},
	urldate = {2018-04-24},
	file = {Snapshot:/home/havard/Zotero/storage/RXZEC43W/handwrittenmathsymbols.html:text/html}
}

@article{chung_empirical_2014,
	title = {Empirical evaluation of gated recurrent neural networks on sequence modeling},
	journaltitle = {{arXiv} preprint {arXiv}:1412.3555},
	author = {Chung, Junyoung and Gulcehre, Caglar and Cho, {KyungHyun} and Bengio, Yoshua},
	date = {2014}
}

@article{cho_learning_2014,
	title = {Learning phrase representations using {RNN} encoder-decoder for statistical machine translation},
	journaltitle = {{arXiv} preprint {arXiv}:1406.1078},
	author = {Cho, Kyunghyun and Van Merriënboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
	date = {2014}
}

@article{zanibbi_recognition_2012,
	title = {Recognition and retrieval of mathematical expressions},
	volume = {15},
	issn = {1433-2833, 1433-2825},
	url = {http://link.springer.com/10.1007/s10032-011-0174-4},
	doi = {10.1007/s10032-011-0174-4},
	pages = {331--357},
	number = {4},
	journaltitle = {International Journal on Document Analysis and Recognition ({IJDAR})},
	author = {Zanibbi, Richard and Blostein, Dorothea},
	urldate = {2018-04-25},
	date = {2012-12},
	langid = {english},
	file = {Zanibbi and Blostein - 2012 - Recognition and retrieval of mathematical expressi.pdf:/home/havard/Zotero/storage/S82PDAQ5/Zanibbi and Blostein - 2012 - Recognition and retrieval of mathematical expressi.pdf:application/pdf}
}

@online{kostadinov_understanding_2017,
	title = {Understanding {GRU} networks},
	url = {https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be},
	abstract = {In this article, I will try to give a fairly simple and understandable explanation of one really fascinating type of neural network…},
	titleaddon = {Towards Data Science},
	author = {Kostadinov, Simeon},
	urldate = {2018-04-25},
	date = {2017-12-16},
	file = {Snapshot:/home/havard/Zotero/storage/J59272XU/understanding-gru-networks-2ef37df6c9be.html:text/html}
}

@online{weisstein_eigen_????,
	title = {Eigen Decomposition},
	rights = {Copyright 1999-2018 Wolfram Research, Inc.  See http://mathworld.wolfram.com/about/terms.html for a full terms of use statement.},
	url = {http://mathworld.wolfram.com/EigenDecomposition.html},
	abstract = {The matrix decomposition of a square matrix A into so-called eigenvalues and eigenvectors is an extremely important one. This decomposition generally goes under the name "matrix diagonalization." However, this moniker is less than optimal, since the process being described is really the decomposition of a matrix into a product of three other matrices, only one of which is diagonal, and also because all other standard types of matrix decomposition use the term...},
	type = {Text},
	author = {Weisstein, Eric W.},
	urldate = {2018-04-25},
	langid = {english},
	file = {Snapshot:/home/havard/Zotero/storage/48398PGL/EigenDecomposition.html:text/html}
}

@article{matsakis_recognition_????,
	title = {Recognition of Handwritten Mathematical Expressions},
	abstract = {In recent years, the recognition of handwritten mathematical expressions has recieved an increasing amount of attention in pattern recognition research. The diversity of approaches to the problem and the lack of a commercially viable system, however, indicate that there is still much research to be done in this area. In this thesis, I will describe an on-line approach for converting a handwritten mathematical expression into an equivalent expression in a typesetting command language such as {TEX} or {MathML}, as well as a feedback-oriented user interface which can make errors more tolerable to the end user since they can be quickly corrected.},
	pages = {59},
	author = {Matsakis, Nicholas E},
	langid = {english},
	file = {Matsakis - Recognition of Handwritten Mathematical Expression.pdf:/home/havard/Zotero/storage/QD4EGLYX/Matsakis - Recognition of Handwritten Mathematical Expression.pdf:application/pdf}
}

@online{_backpropagation_????,
	title = {Backpropagation {\textbar} Brilliant Math \& Science Wiki},
	url = {https://brilliant.org/wiki/backpropagation/},
	abstract = {Backpropagation, short for \&quot;backward propagation of errors,\&quot; is an algorithm for supervised learning of artificial neural networks using gradient descent. Given an artificial neural network and an error function, the method calculates the gradient of the error function with respect to the neural network\&\#39;s weights. It is a generalization of the delta rule for perceptrons to multilayer feedforward neural networks. The \&quot;backwards\&quot; part of the name stems from the fact that calculation of the ...},
	urldate = {2018-04-30},
	langid = {english},
	file = {Snapshot:/home/havard/Zotero/storage/UT4F9ELA/backpropagation.html:text/html}
}

@article{downing_backpropagation:_????,
	title = {Backpropagation: The Good, the Bad and the Ugly},
	pages = {45},
	author = {Downing, Keith L},
	langid = {english},
	file = {Downing - Backpropagation The Good, the Bad and the Ugly.pdf:/home/havard/Zotero/storage/VU9ZYCTZ/Downing - Backpropagation The Good, the Bad and the Ugly.pdf:application/pdf}
}