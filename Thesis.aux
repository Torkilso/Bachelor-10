\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\abx@aux@refcontext{none/global//global/global}
\providecommand\@newglossary[4]{}
\@newglossary{main}{glg}{gls}{glo}
\providecommand\@glsorder[1]{}
\providecommand\@istfilename[1]{}
\@istfilename{Thesis.ist}
\@glsorder{word}
\@writefile{toc}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lof}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lot}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\babel@aux{english}{}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{Preface}{i}{dummy.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\vspace  {1em}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{Assignment}{ii}{dummy.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\vspace  {1em}}
\newlabel{assignment_page}{{2}{ii}{}{dummy.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{Abstract}{iii}{dummy.3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\vspace  {1em}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{List of Figures}{vii}{dummy.5}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{List of Tables}{x}{dummy.7}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\vspace  {2em}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.10}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {1.1}Motivation}{1}{section.11}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {1.2}Problem definition}{1}{section.12}}
\newlabel{problem_definition}{{1.2}{1}{Problem definition}{section.12}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {1.3}Structure of the report}{1}{section.13}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}Chapter 1 Introduction}{1}{subsection.14}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}Chapter 2 Theory}{2}{subsection.15}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.3}Chapter 3 Technology and Method}{2}{subsection.16}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.4}Chapter 4 Results}{2}{subsection.17}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.5}Chapter 5 Discussion}{2}{subsection.21}}
\abx@aux@cite{_mit_????}
\abx@aux@segm{0}{0}{_mit_????}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.6}Chapter 6 Conclusion}{3}{subsection.25}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.7}Attachments}{3}{subsection.26}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {1.4}Target audience}{3}{section.27}}
\abx@aux@cite{thoma_-line_2015}
\abx@aux@segm{0}{0}{thoma_-line_2015}
\abx@aux@cite{lu_recognition_2015}
\abx@aux@segm{0}{0}{lu_recognition_2015}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.1}Application}{4}{subsection.28}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {1.5}Purpose}{4}{section.29}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {1.6}Abbreviations}{5}{section.30}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{table:abbreviations}{{\caption@xref {table:abbreviations}{ on input line 162}}{5}{Abbreviations}{table.caption.31}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {1.1}{\ignorespaces A list of abbreviations used, in alphabetical order.\relax }}{5}{table.caption.31}}
\abx@aux@cite{simon_off-line_1992}
\abx@aux@segm{0}{0}{simon_off-line_1992}
\abx@aux@cite{mori_historical_1992}
\abx@aux@segm{0}{0}{mori_historical_1992}
\abx@aux@segm{0}{0}{simon_off-line_1992}
\abx@aux@segm{0}{0}{mori_historical_1992}
\abx@aux@cite{fukushima_handwritten_????}
\abx@aux@segm{0}{0}{fukushima_handwritten_????}
\abx@aux@segm{0}{0}{mori_historical_1992}
\abx@aux@cite{priya_online_2016}
\abx@aux@segm{0}{0}{priya_online_2016}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {2}Theory}{6}{chapter.32}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.1}Handwriting recognition}{6}{section.33}}
\newlabel{handwriting_recognition}{{2.1}{6}{Handwriting recognition}{section.33}{}}
\abx@aux@segm{0}{0}{priya_online_2016}
\abx@aux@cite{huang_preprocessing_2007}
\abx@aux@segm{0}{0}{huang_preprocessing_2007}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Main approaches}{7}{subsection.34}}
\newlabel{main_approaches}{{2.1.1}{7}{Main approaches}{subsection.34}{}}
\newlabel{fig:online_data}{{2.1a}{7}{\relax }{figure.caption.35}{}}
\newlabel{sub@fig:online_data}{{a}{7}{\relax }{figure.caption.35}{}}
\newlabel{fig:offline_data}{{2.1b}{7}{\relax }{figure.caption.35}{}}
\newlabel{sub@fig:offline_data}{{b}{7}{\relax }{figure.caption.35}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces A: Online recognition data with coordinate samples. B: Offline recognition data (only an image).\relax }}{7}{figure.caption.35}}
\newlabel{fig:online_offline_comparison}{{2.1}{7}{A: Online recognition data with coordinate samples. B: Offline recognition data (only an image).\relax }{figure.caption.35}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Common steps in handwriting recognition}{7}{subsection.36}}
\abx@aux@segm{0}{0}{huang_preprocessing_2007}
\abx@aux@segm{0}{0}{priya_online_2016}
\abx@aux@cite{h_algorithms_2011}
\abx@aux@segm{0}{0}{h_algorithms_2011}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2.1}Preprocessing}{8}{subsubsection.37}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Figure of calculating the perpendicular distance to a line drawn between two endpoints\relax }}{8}{figure.caption.38}}
\newlabel{fig:my_label}{{2.2}{8}{Figure of calculating the perpendicular distance to a line drawn between two endpoints\relax }{figure.caption.38}{}}
\newlabel{ramer_douglas_peucker}{{2.1.2.1}{9}{Preprocessing}{figure.caption.38}{}}
\newlabel{eqn:scale_linear_by_column}{{2.1}{9}{Preprocessing}{equation.39}{}}
\newlabel{scale_linear_by_column}{{2.1.2.1}{9}{Preprocessing}{equation.39}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2.2}Segmentation}{9}{subsubsection.40}}
\newlabel{Segmentation}{{2.1.2.2}{9}{Segmentation}{subsubsection.40}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces The segmentation problem deals with segmenting symbols from traces. The sample above has three traces, one for the "2" and two for the "x". How they are segmented can lead to many different outcomes.\relax }}{9}{figure.caption.41}}
\newlabel{fig:segmentation_1}{{2.3}{9}{The segmentation problem deals with segmenting symbols from traces. The sample above has three traces, one for the "2" and two for the "x". How they are segmented can lead to many different outcomes.\relax }{figure.caption.41}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2.3}Classification of symbols}{10}{subsubsection.42}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2.4}Postprocessing}{10}{subsubsection.43}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.2}Recognition of handwritten mathematics}{10}{section.44}}
\newlabel{recognition_of_handwritten_mathematics}{{2.2}{10}{Recognition of handwritten mathematics}{section.44}{}}
\abx@aux@cite{mouchere_icfhr2016_2016}
\abx@aux@segm{0}{0}{mouchere_icfhr2016_2016}
\abx@aux@segm{0}{0}{mouchere_icfhr2016_2016}
\abx@aux@cite{mouchere_advancing_2016}
\abx@aux@segm{0}{0}{mouchere_advancing_2016}
\abx@aux@segm{0}{0}{mouchere_icfhr2016_2016}
\abx@aux@cite{chee_ink_2011}
\abx@aux@segm{0}{0}{chee_ink_2011}
\newlabel{steps_in_math_recog}{{\caption@xref {steps_in_math_recog}{ on input line 163}}{11}{Recognition of handwritten mathematics}{figure.caption.45}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Previous work and existing solutions}{11}{subsection.46}}
\newlabel{previous_work_existing_solutions}{{2.2.1}{11}{Previous work and existing solutions}{subsection.46}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}InkML}{11}{subsection.47}}
\abx@aux@cite{murphy_machine_2012}
\abx@aux@segm{0}{0}{murphy_machine_2012}
\abx@aux@cite{cline_predictive_2017}
\abx@aux@segm{0}{0}{cline_predictive_2017}
\abx@aux@cite{goodfellow_deep_2016}
\abx@aux@segm{0}{0}{goodfellow_deep_2016}
\newlabel{lst:InkML_ex}{{2.1}{12}{\textbf {\gls {InkML}} example of three traces, each with an file-unique id. In addition to traces, we have listed a tracegroup, which specifies what and where the traces belong to. Truths are used when providing labels to use in supervised learning, which is explained later in this chapter}{lstlisting.48}{}}
\@writefile{lol}{\defcounter {refsection}{0}\relax }\@writefile{lol}{\contentsline {lstlisting}{\numberline {2.1}\textbf  {\gls {InkML}} example of three traces, each with an file-unique id. In addition to traces, we have listed a tracegroup, which specifies what and where the traces belong to. Truths are used when providing labels to use in supervised learning, which is explained later in this chapter.}{12}{lstlisting.48}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.3}Machine learning}{12}{section.65}}
\newlabel{machine_learning}{{2.3}{12}{Machine learning}{section.65}{}}
\abx@aux@segm{0}{0}{goodfellow_deep_2016}
\abx@aux@cite{_cs231n_????}
\abx@aux@segm{0}{0}{_cs231n_????}
\abx@aux@cite{_multi-layer_????}
\abx@aux@segm{0}{0}{_multi-layer_????}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Supervised learning}{13}{subsection.66}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.4}Artificial neural networks}{13}{section.67}}
\newlabel{artificial_neural_networks}{{2.4}{13}{Artificial neural networks}{section.67}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Diagram of a single artificial neuron. $W_{i}$ is the weight of input edge i, $x_{i}$ is the value coming from edge i and b is the bias.\relax }}{13}{figure.caption.68}}
\newlabel{fig:single_neuron}{{2.4}{13}{Diagram of a single artificial neuron. $W_{i}$ is the weight of input edge i, $x_{i}$ is the value coming from edge i and b is the bias.\relax }{figure.caption.68}{}}
\abx@aux@cite{smith_scientist_1997}
\abx@aux@segm{0}{0}{smith_scientist_1997}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Diagram of a single layered neural network. It contains three neurons in the input layer, three neurons in the hidden layer, and a single neuron in the output layer. \relax }}{14}{figure.caption.69}}
\newlabel{fig:single_layered_neural_network}{{2.5}{14}{Diagram of a single layered neural network. It contains three neurons in the input layer, three neurons in the hidden layer, and a single neuron in the output layer. \relax }{figure.caption.69}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces \relax }}{14}{figure.caption.70}}
\newlabel{fig:calculating_output_one}{{2.6}{14}{\relax }{figure.caption.70}{}}
\abx@aux@cite{mcculloch_logical_1943}
\abx@aux@segm{0}{0}{mcculloch_logical_1943}
\abx@aux@cite{jain_artificial_1996}
\abx@aux@segm{0}{0}{jain_artificial_1996}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces The final output is calculated\relax }}{15}{figure.caption.71}}
\newlabel{fig:calculating_output_two}{{2.7}{15}{The final output is calculated\relax }{figure.caption.71}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Activation functions}{15}{subsection.72}}
\newlabel{activation_functions}{{2.4.1}{15}{Activation functions}{subsection.72}{}}
\abx@aux@cite{leshno_multilayer_1993}
\abx@aux@segm{0}{0}{leshno_multilayer_1993}
\abx@aux@segm{0}{0}{jain_artificial_1996}
\abx@aux@cite{sharma_understanding_2018}
\abx@aux@segm{0}{0}{sharma_understanding_2018}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Three activation functions plotted on the interval [-4,4].\relax }}{16}{figure.caption.73}}
\abx@aux@segm{0}{0}{sharma_understanding_2018}
\abx@aux@segm{0}{0}{smith_scientist_1997}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Figure of the exclusive-OR problem, by the capabilities of a model with non-linear activation functions versus no activation function.\relax }}{17}{figure.caption.74}}
\newlabel{fig:exclusive-OR problem}{{2.9}{17}{Figure of the exclusive-OR problem, by the capabilities of a model with non-linear activation functions versus no activation function.\relax }{figure.caption.74}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces Figure of how example input data is transformed through two different activation functions. Data flows from left to right, with output from the previous graph used as input to the next.\relax }}{17}{figure.caption.75}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.1.1}ReLU}{17}{subsubsection.76}}
\newlabel{eqn:relu}{{2.2}{17}{ReLU}{equation.77}{}}
\abx@aux@cite{krizhevsky_imagenet_2012}
\abx@aux@segm{0}{0}{krizhevsky_imagenet_2012}
\abx@aux@cite{zeiler_rectified_2013}
\abx@aux@segm{0}{0}{zeiler_rectified_2013}
\abx@aux@cite{_cs231n_????-1}
\abx@aux@segm{0}{0}{_cs231n_????-1}
\abx@aux@cite{_neural_2018}
\abx@aux@segm{0}{0}{_neural_2018}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.1.2}Softmax}{18}{subsubsection.78}}
\newlabel{eqn:softmax}{{2.3}{18}{Softmax}{equation.79}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.1.3}Sigmoid}{18}{subsubsection.80}}
\newlabel{eqn:sigmoid}{{2.4}{18}{Sigmoid}{equation.81}{}}
\abx@aux@segm{0}{0}{_neural_2018}
\abx@aux@cite{brownlee_how_2017}
\abx@aux@segm{0}{0}{brownlee_how_2017}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.1.4}Tanh}{19}{subsubsection.82}}
\newlabel{eqn:tanh}{{2.5}{19}{Tanh}{equation.83}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Loss functions}{19}{subsection.84}}
\newlabel{loss_function}{{2.4.2}{19}{Loss functions}{subsection.84}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.2.1}Crossentropy}{19}{subsubsection.86}}
\newlabel{categorical-crossentropy}{{2.4.2.1}{19}{Crossentropy}{subsubsection.86}{}}
\abx@aux@cite{werbos_beyond_1974}
\abx@aux@segm{0}{0}{werbos_beyond_1974}
\abx@aux@cite{rumelhart_learning_1986}
\abx@aux@segm{0}{0}{rumelhart_learning_1986}
\newlabel{eqn:catcross_ex1}{{2.7}{20}{Crossentropy}{equation.87}{}}
\newlabel{eqn:catcross_ex2}{{2.8}{20}{Crossentropy}{equation.88}{}}
\newlabel{eqn:catcross_ex3}{{2.9}{20}{Crossentropy}{equation.89}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}Training neural networks}{20}{subsection.91}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.3.1}Training with backpropagation}{20}{subsubsection.92}}
\newlabel{training_with_backpropagation}{{2.4.3.1}{20}{Training with backpropagation}{subsubsection.92}{}}
\abx@aux@cite{srivastava_dropout:_2014}
\abx@aux@segm{0}{0}{srivastava_dropout:_2014}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces Finding the error with mean squared error and sending it backwards.\relax }}{21}{figure.caption.93}}
\newlabel{fig:backprop_one}{{2.11}{21}{Finding the error with mean squared error and sending it backwards.\relax }{figure.caption.93}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces Using error from the hidden units to calculate modification for the connected weights. The same process is repeated for the other hidden units.\relax }}{21}{figure.caption.94}}
\newlabel{fig:backprop_two}{{2.12}{21}{Using error from the hidden units to calculate modification for the connected weights. The same process is repeated for the other hidden units.\relax }{figure.caption.94}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.3.2}Backpropagating through time}{21}{subsubsection.95}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.4}Dropout}{21}{subsection.96}}
\newlabel{dead-neurons}{{2.4.4}{21}{Dropout}{subsection.96}{}}
\abx@aux@segm{0}{0}{srivastava_dropout:_2014}
\newlabel{fig:net_without_dropout}{{2.13a}{22}{\relax }{figure.caption.97}{}}
\newlabel{sub@fig:net_without_dropout}{{a}{22}{\relax }{figure.caption.97}{}}
\newlabel{fig:net_with_dropout}{{2.13b}{22}{\relax }{figure.caption.97}{}}
\newlabel{sub@fig:net_with_dropout}{{b}{22}{\relax }{figure.caption.97}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.13}{\ignorespaces Neural networks without (A) and with (B) dropout applied. The crossed circles are the dropped units.\relax }}{22}{figure.caption.97}}
\newlabel{fig:dropout_comparison}{{2.13}{22}{Neural networks without (A) and with (B) dropout applied. The crossed circles are the dropped units.\relax }{figure.caption.97}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.5}Common problems}{22}{subsection.98}}
\newlabel{vanishing-gradient}{{2.4.5}{22}{Common problems}{subsection.98}{}}
\newlabel{exploding-gradient}{{2.4.5}{22}{Common problems}{subsection.98}{}}
\abx@aux@cite{weisstein_eigen_????}
\abx@aux@segm{0}{0}{weisstein_eigen_????}
\abx@aux@segm{0}{0}{goodfellow_deep_2016}
\abx@aux@segm{0}{0}{_cs231n_????}
\abx@aux@segm{0}{0}{goodfellow_deep_2016}
\abx@aux@segm{0}{0}{goodfellow_deep_2016}
\abx@aux@cite{lecun_generalization_1989}
\abx@aux@segm{0}{0}{lecun_generalization_1989}
\newlabel{eqn:mat_decomp}{{2.11}{23}{Common problems}{equation.99}{}}
\newlabel{eqn:mat_decomp_t}{{2.12}{23}{Common problems}{equation.100}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.5}Feedforward neural networks}{23}{section.101}}
\abx@aux@segm{0}{0}{goodfellow_deep_2016}
\abx@aux@segm{0}{0}{goodfellow_deep_2016}
\abx@aux@segm{0}{0}{goodfellow_deep_2016}
\abx@aux@segm{0}{0}{goodfellow_deep_2016}
\abx@aux@segm{0}{0}{goodfellow_deep_2016}
\abx@aux@segm{0}{0}{goodfellow_deep_2016}
\abx@aux@segm{0}{0}{goodfellow_deep_2016}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Convolutional Networks}{24}{subsection.102}}
\newlabel{eqn:conv}{{\caption@xref {eqn:conv}{ on input line 981}}{24}{Convolutional Networks}{figure.caption.103}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.14}{\ignorespaces \cite {goodfellow_deep_2016}[p.~322] Example convolution which averages sensor readings and weights the most recent sensor readings. w(a) is the weighting function, x(t) is the position of the spaceship at time t. The result of this is a new function s, which gives us a smoothed estimate\relax }}{24}{figure.caption.103}}
\newlabel{fig:conv_asterisk}{{\caption@xref {fig:conv_asterisk}{ on input line 989}}{24}{Convolutional Networks}{figure.caption.105}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.15}{\ignorespaces Asterisk notation of a convolution.\relax }}{24}{figure.caption.105}}
\abx@aux@segm{0}{0}{goodfellow_deep_2016}
\abx@aux@segm{0}{0}{goodfellow_deep_2016}
\abx@aux@segm{0}{0}{goodfellow_deep_2016}
\abx@aux@segm{0}{0}{goodfellow_deep_2016}
\abx@aux@cite{nielsen_neural_2015}
\abx@aux@segm{0}{0}{nielsen_neural_2015}
\abx@aux@segm{0}{0}{nielsen_neural_2015}
\newlabel{fig:disc_conv}{{\caption@xref {fig:disc_conv}{ on input line 1001}}{25}{Convolutional Networks}{figure.caption.107}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.16}{\ignorespaces \cite {goodfellow_deep_2016}[p.~323] definition on the discrete convolution.\relax }}{25}{figure.caption.107}}
\newlabel{fig:disc_conv2d}{{\caption@xref {fig:disc_conv2d}{ on input line 1009}}{25}{Convolutional Networks}{figure.caption.109}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.17}{\ignorespaces \cite {goodfellow_deep_2016}[p.~323] the two dimensional kernel in both it's original form in addition to the version with the flipped kernel. The last version is a result of \textbf  {flipping} the kernel relative to it's input.\relax }}{25}{figure.caption.109}}
\newlabel{fig:cross_corr}{{\caption@xref {fig:cross_corr}{ on input line 1024}}{25}{Convolutional Networks}{equation.113}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.18}{\ignorespaces \cite {goodfellow_deep_2016}[p.~324] the \textbf  {cross-correlation} function.\relax }}{25}{figure.caption.112}}
\abx@aux@cite{zhou_computation_1988}
\abx@aux@segm{0}{0}{zhou_computation_1988}
\abx@aux@segm{0}{0}{goodfellow_deep_2016}
\abx@aux@segm{0}{0}{nielsen_neural_2015}
\abx@aux@segm{0}{0}{rumelhart_learning_1986}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.19}{\ignorespaces This figure from \cite {nielsen_neural_2015} shows how a convolution layer works with a 5x5 kernel, also referred to as a filter or feature detector. In this example, our input is 28x28, if we apply a 5x5 kernel we will end up with 24x24 neurons in the hidden layer. Available from http://neuralnetworksanddeeplearning.com/images/tikz45.png, 23.04.2018\relax }}{26}{figure.caption.114}}
\newlabel{fig:kernel_applied}{{2.19}{26}{This figure from \cite {nielsen_neural_2015} shows how a convolution layer works with a 5x5 kernel, also referred to as a filter or feature detector. In this example, our input is 28x28, if we apply a 5x5 kernel we will end up with 24x24 neurons in the hidden layer. Available from http://neuralnetworksanddeeplearning.com/images/tikz45.png, 23.04.2018\relax }{figure.caption.114}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.6}Recurrent neural networks}{26}{section.115}}
\abx@aux@segm{0}{0}{goodfellow_deep_2016}
\abx@aux@segm{0}{0}{goodfellow_deep_2016}
\abx@aux@segm{0}{0}{nielsen_neural_2015}
\abx@aux@cite{hochreiter_long_1997}
\abx@aux@segm{0}{0}{hochreiter_long_1997}
\abx@aux@cite{gers_learning_1999}
\abx@aux@segm{0}{0}{gers_learning_1999}
\abx@aux@cite{chung_empirical_2014}
\abx@aux@segm{0}{0}{chung_empirical_2014}
\abx@aux@segm{0}{0}{chung_empirical_2014}
\abx@aux@segm{0}{0}{gers_learning_1999}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.1}Long short-term memory}{27}{subsection.116}}
\newlabel{theory-LSTM}{{2.6.1}{27}{Long short-term memory}{subsection.116}{}}
\abx@aux@cite{_understanding_2015}
\abx@aux@segm{0}{0}{_understanding_2015}
\abx@aux@segm{0}{0}{_understanding_2015}
\abx@aux@segm{0}{0}{gers_learning_1999}
\abx@aux@segm{0}{0}{_understanding_2015}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.20}{\ignorespaces Model of a memory block with three memory cells, available from http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png at 10-05-2018. The center cell includes a visualization of the different parts of the cell, and how the information flows inside the cell. The cell's input values are the current state of the memory block $C_{t-1}$ (top left), the output of the previous cell $h_{t-1}$ (bottom left), and the current input value $x_t$. The upper line is the memory block's state, which flows through each cell of the block. The yellow squares are neural network layers with a specified activation function. \cite {_understanding_2015})\relax }}{28}{figure.caption.117}}
\newlabel{fig:lstm_cells}{{2.20}{28}{Model of a memory block with three memory cells, available from http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png at 10-05-2018. The center cell includes a visualization of the different parts of the cell, and how the information flows inside the cell. The cell's input values are the current state of the memory block $C_{t-1}$ (top left), the output of the previous cell $h_{t-1}$ (bottom left), and the current input value $x_t$. The upper line is the memory block's state, which flows through each cell of the block. The yellow squares are neural network layers with a specified activation function. \cite {_understanding_2015})\relax }{figure.caption.117}{}}
\abx@aux@cite{cho_learning_2014}
\abx@aux@segm{0}{0}{cho_learning_2014}
\abx@aux@segm{0}{0}{chung_empirical_2014}
\abx@aux@cite{kostadinov_understanding_2017}
\abx@aux@segm{0}{0}{kostadinov_understanding_2017}
\abx@aux@segm{0}{0}{kostadinov_understanding_2017}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.2}Gated Recurrent Unit}{29}{subsection.118}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.21}{\ignorespaces A model of a Gated Recurrent Unit from https://cdn-images-1.medium.com/max/800/1*6eNTqLzQ08AABo-STFNiBw.png at 10-05-2018. $h_{t-1}$ is the output of previous cell, $h_t$ is the output of current cell. $z_t$ is the output from the update gate. $r_t$ is the output of the reset gate. $x_t$ is the input at current timestep. \cite {kostadinov_understanding_2017}\relax }}{29}{figure.caption.119}}
\newlabel{fig:gru-single-cell}{{2.21}{29}{A model of a Gated Recurrent Unit from https://cdn-images-1.medium.com/max/800/1*6eNTqLzQ08AABo-STFNiBw.png at 10-05-2018. $h_{t-1}$ is the output of previous cell, $h_t$ is the output of current cell. $z_t$ is the output from the update gate. $r_t$ is the output of the reset gate. $x_t$ is the input at current timestep. \cite {kostadinov_understanding_2017}\relax }{figure.caption.119}{}}
\abx@aux@segm{0}{0}{kostadinov_understanding_2017}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {3}Technology and method}{31}{chapter.120}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3.1}Introduction}{31}{section.121}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3.2}Technology}{31}{section.122}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Version control}{31}{subsection.123}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Collaboration tools}{31}{subsection.124}}
\abx@aux@cite{chollet_keras_2015}
\abx@aux@segm{0}{0}{chollet_keras_2015}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Python and frameworks}{32}{subsection.125}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3.3}Architecture}{32}{section.126}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3.4}Preparing data}{33}{section.127}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Traces}{33}{subsection.128}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces A square root which has not been through preprocessing.\relax }}{34}{figure.caption.129}}
\newlabel{fig:sqrt_not_processed}{{3.1}{34}{A square root which has not been through preprocessing.\relax }{figure.caption.129}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces A square root after trace preprocessing.\relax }}{35}{figure.caption.130}}
\newlabel{fig:sqrt_processed}{{3.2}{35}{A square root after trace preprocessing.\relax }{figure.caption.130}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Images}{35}{subsection.131}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces An 8x10 matrix representing a pixel grid of a square root symbol.\relax }}{36}{figure.caption.132}}
\newlabel{fig:sqrt_matrix}{{3.3}{36}{An 8x10 matrix representing a pixel grid of a square root symbol.\relax }{figure.caption.132}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces The resulting square root from the preprocessing done in previous steps. The generated image is 26x26 pixels.\relax }}{36}{figure.caption.133}}
\newlabel{fig:sqrt_img}{{3.4}{36}{The resulting square root from the preprocessing done in previous steps.\\The generated image is 26x26 pixels.\relax }{figure.caption.133}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3.5}The recognition system}{36}{section.134}}
\newlabel{the_recognintion_system}{{3.5}{36}{The recognition system}{section.134}{}}
\abx@aux@cite{_recurrent_????}
\abx@aux@segm{0}{0}{_recurrent_????}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Preprocessing and segmentation}{37}{subsection.135}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Model architecture}{37}{subsection.136}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.2.1}RNN model}{37}{subsubsection.137}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces A visualisation of the layer architecture in the RNN model.\relax }}{38}{figure.caption.138}}
\newlabel{fig:RNN__model_visualization_1}{{3.5}{38}{A visualisation of the layer architecture in the RNN model.\relax }{figure.caption.138}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.2.2}CNN model}{38}{subsubsection.139}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces A visualisation of the layer architecture in the CNN model.\relax }}{39}{figure.caption.140}}
\newlabel{fig:RNN__model_visualization_2}{{3.6}{39}{A visualisation of the layer architecture in the CNN model.\relax }{figure.caption.140}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.2.3}Combined model}{39}{subsubsection.141}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces A visualisation of the layer architecture in the combined model.\relax }}{40}{figure.caption.142}}
\newlabel{fig:RNN__model_visualization_3}{{3.7}{40}{A visualisation of the layer architecture in the combined model.\relax }{figure.caption.142}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.3}Interpretation and context search}{40}{subsection.143}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces An input expression and the resulting context tree.\relax }}{40}{figure.caption.144}}
\newlabel{fig:interpretation-tree1}{{3.8}{40}{An input expression and the resulting context tree.\relax }{figure.caption.144}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.3.1}The main function}{41}{subsubsection.145}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces Flowchart of the order and recursion.\relax }}{42}{figure.caption.151}}
\newlabel{fig:interpretation_flowchart}{{3.9}{42}{Flowchart of the order and recursion.\relax }{figure.caption.151}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.3.2}Square roots}{42}{subsubsection.152}}
\newlabel{interpretation-square-roots}{{3.5.3.2}{42}{Square roots}{subsubsection.152}{}}
\newlabel{fig:interpretation}{{\caption@xref {fig:interpretation}{ on input line 377}}{43}{Square roots}{figure.caption.153}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces Search area of square roots.\relax }}{43}{figure.caption.153}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces An expression with a square root inside a square root and the resulting context tree.\relax }}{43}{figure.caption.154}}
\newlabel{fig:segmentation}{{3.11}{43}{An expression with a square root inside a square root and the resulting context tree.\relax }{figure.caption.154}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.3.3}Fractions}{43}{subsubsection.155}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3.12}{\ignorespaces The search areas of a fraction expression with recursion and the corresponding context tree. The widest minus sign is considered the root fraction.\relax }}{44}{figure.caption.156}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.3.4}Exponents}{44}{subsubsection.157}}
\newlabel{interpretation-exponents}{{3.5.3.4}{44}{Exponents}{subsubsection.157}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3.13}{\ignorespaces Example of a power group and visualization of the first and second rule.\relax }}{45}{figure.caption.158}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3.14}{\ignorespaces Power groups and their corresponding context tree.\relax }}{45}{figure.caption.159}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.3.5}Special case symbols}{45}{subsubsection.160}}
\newlabel{interpretation-special-symbols}{{3.5.3.5}{45}{Special case symbols}{subsubsection.160}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.4}Converting to \LaTeX  }{46}{subsection.161}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3.6}The project process}{46}{section.162}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3.7}Teamwork and roles}{47}{section.163}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {4}Results}{48}{chapter.164}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {4.1}Scientific results}{48}{section.165}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Training accuracy from the CNN model with 44000 data points evaluated for 40 epochs.\relax }}{49}{figure.caption.166}}
\newlabel{fig:CNN_44000}{{4.1}{49}{Training accuracy from the CNN model with 44000 data points evaluated for 40 epochs.\relax }{figure.caption.166}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Training accguracy from the RNN model with 44000 data points evaluated for 40 epochs.\relax }}{49}{figure.caption.167}}
\newlabel{fig:RNN_44000}{{4.2}{49}{Training accguracy from the RNN model with 44000 data points evaluated for 40 epochs.\relax }{figure.caption.167}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Training accuracy from the combined CNN and RNN model with 44000 datapoints. Evaluated for 40 epochs.\relax }}{50}{figure.caption.168}}
\newlabel{fig:combined_CNN_RNN_44000}{{4.3}{50}{Training accuracy from the combined CNN and RNN model with 44000 datapoints. Evaluated for 40 epochs.\relax }{figure.caption.168}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Snapshot of accuracy in percent on the train dataset for all models at epoch number 10, 20, 30 and 40\relax }}{50}{figure.caption.169}}
\newlabel{fig:table_train_dataset}{{4.4}{50}{Snapshot of accuracy in percent on the train dataset for all models at epoch number 10, 20, 30 and 40\relax }{figure.caption.169}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Snapshot of accuracy in percent on the test dataset for all models at epoch number 10, 20, 30 and 40\relax }}{50}{figure.caption.170}}
\newlabel{fig:table_test_dataset}{{4.5}{50}{Snapshot of accuracy in percent on the test dataset for all models at epoch number 10, 20, 30 and 40\relax }{figure.caption.170}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Snapshot of accuracy in percent on the real dataset for all models at epoch number 10, 20, 30 and 40\relax }}{51}{figure.caption.171}}
\newlabel{fig:table_real_dataset}{{4.6}{51}{Snapshot of accuracy in percent on the real dataset for all models at epoch number 10, 20, 30 and 40\relax }{figure.caption.171}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {4.2}Engineering results}{51}{section.172}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces Example of the graphical user interface. A simple HTML5 canvas is used to perform input, currently it does a HTTP post request to a back end to perform classification of buffer. The HTTP post method could be swapped with a WebSocket. The result is shown at the top right corner in human readable form, directly under the result is the result in \LaTeX  , this is useful to further use it in some other API, for example Wolfram Alpha. Under the \LaTeX  is the probabilities of either a chosen symbol or the latest drawn.\relax }}{51}{figure.caption.173}}
\newlabel{fig:predictor_example}{{4.7}{51}{Example of the graphical user interface. A simple HTML5 canvas is used to perform input, currently it does a HTTP post request to a back end to perform classification of buffer. The HTTP post method could be swapped with a WebSocket. The result is shown at the top right corner in human readable form, directly under the result is the result in \LaTeX , this is useful to further use it in some other API, for example Wolfram Alpha. Under the \LaTeX is the probabilities of either a chosen symbol or the latest drawn.\relax }{figure.caption.173}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}User Interface}{52}{subsection.174}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {4.3}Administrative results}{52}{section.175}}
\abx@aux@segm{0}{0}{_recurrent_????}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {5}Discussion}{53}{chapter.176}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {5.1}Scientific result}{53}{section.177}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}}{54}{subsection.178}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {5.2}The final product}{54}{section.179}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}}{55}{subsection.180}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}Process, approach and technology}{55}{subsection.181}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.3}Strengths and weaknesses}{55}{subsection.182}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {5.3}}{56}{section.183}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}Ethics}{56}{subsection.184}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {5.4}The development process}{56}{section.185}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.1}Teamwork}{56}{subsection.186}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.2}Professional ethics}{56}{subsection.187}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\vspace  {2em}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {A}System documentation}{57}{appendix.188}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\vspace  {2em}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{Bibliography}{58}{dummy.189}}
\abx@aux@defaultrefcontext{0}{_mit_????}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{thoma_-line_2015}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{lu_recognition_2015}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{simon_off-line_1992}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{mori_historical_1992}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{fukushima_handwritten_????}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{priya_online_2016}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{huang_preprocessing_2007}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{h_algorithms_2011}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{mouchere_icfhr2016_2016}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{mouchere_advancing_2016}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{chee_ink_2011}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{murphy_machine_2012}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{cline_predictive_2017}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{goodfellow_deep_2016}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{_cs231n_????}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{_multi-layer_????}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{smith_scientist_1997}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{mcculloch_logical_1943}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{jain_artificial_1996}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{leshno_multilayer_1993}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{sharma_understanding_2018}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{krizhevsky_imagenet_2012}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{zeiler_rectified_2013}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{_cs231n_????-1}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{_neural_2018}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{brownlee_how_2017}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{werbos_beyond_1974}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{rumelhart_learning_1986}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{srivastava_dropout:_2014}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{weisstein_eigen_????}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{lecun_generalization_1989}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{nielsen_neural_2015}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{zhou_computation_1988}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{hochreiter_long_1997}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{gers_learning_1999}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{chung_empirical_2014}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{_understanding_2015}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{cho_learning_2014}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{kostadinov_understanding_2017}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{chollet_keras_2015}{none/global//global/global}
